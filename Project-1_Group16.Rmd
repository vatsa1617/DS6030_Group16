---
title:  |
        | Disaster Relief Project
        | Part One
author: |
        | Group 16
        | Srivatsa Balasubramanyam, Kaya Oguz, Kristina Quintana, Shriya Kuruba

date: June 23, 2025
output:
  bookdown::html_document2:
    number_sections: true
    toc: false
    extra_dependencies: ["float"]
subtitle: |
          | University of Virginia
          | School of Data Science
          | Statistical Learning
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=TRUE, cache=TRUE, autodep=TRUE, fig.align="center")
```

### Load the required library

```{r message=FALSE, warning=FALSE}
rm(list=ls())
library(tidymodels)
library(tidyverse)
library(patchwork)
library(ggplot2)
library(dplyr)
library(themis)
library(discrim)
```

```{r}
#| cache: FALSE
#| message: false
library(doParallel)

cl <- makePSOCKcluster(parallelly::availableCores(omit = 4))
registerDoParallel(cl)
```

**Introduction**

In the immediate aftermath of the January 12, 2010 earthquake in Haiti, nations worldwide responded to the Haitian government’s appeal for aid by pledging funds and dispatching rescue personnel. However, as rescue operations commenced, a critical challenge emerged: the destruction of communications infrastructure made it extremely difficult to determine where to direct emergency supplies and assistance.

**Challenge: Locating Displaced Populations**

With millions displaced and traditional communication channels inoperable, rescue workers needed an effective method to identify the locations of those most in need. It was observed that many displaced individuals were constructing makeshift shelters using blue tarps, which became a key visual indicator of temporary settlements

A team from the Rochester Institute of Technology addressed this challenge by collecting high-resolution, geo-referenced aerial photographs over the affected areas. These images, while rich in detail, represented an enormous volume of data, making manual analysis infeasible given the urgency and scale of the crisis.

**Solution: Statistical Modeling of Aerial Imagery**

To overcome the limitations of manual image analysis, statistical modeling techniques were employed to automate the identification of makeshift shelters. The process involved:

-   Using the color characteristics of blue tarps as the primary indicator of shelter locations.
-   Extracting pixel-level data (Red, Green, Blue values) from the aerial images as predictor variables.
-   Applying supervised learning algorithms to classify image regions as either containing blue tarps (shelters) or not.

**Model Selection and Evaluation**

Many statistical models will be considered for this task:

Models without tuning parameters – Logistic Regression – LDA (Linear Discriminant Analysis) – QDA (Quadratic Discriminant Analysis)

• Models with tuning parameters – KNN (K-nearest neighbor) – Penalized Logistic Regression (elastic net penalty) – Ensemble method: random forest (ranger) or boosting (XGBoost) – Support Vector Machines (SVM) ∗ linear kernel ∗ polynomial kernel ∗ radial basis function kernel

#### Load the Haiti Dataset

```{r message=FALSE, warning=FALSE}
get_holdouts <- function(file) {
  
  infile <- read_lines(file,n_max = 20)
  
  col_row <- infile %>% 
    data.frame() %>% 
    mutate(text = gsub(';','',.)) %>% 
    select(-.)%>% 
    mutate(end = right(text,2) == 'B3',
           row = row_number()) %>% 
    filter(end==T) %>% 
    select(row)
  
  f <- suppressWarnings(read_table(file,skip = col_row$row-1,n_max = 20,col_types = cols()))
  
  long_cols <- c('ID','X','Y','Map_X','Map_Y','Lat','Long','B1','B2','B3')
  short_cols <- c('B1','B2','B3')
  
  if (ncol(f) > 4) {
    final <- read_table(file,skip = col_row$row,col_names = long_cols,col_types = cols())
  } else {
    final <- read_table(file,skip = col_row$row,col_names = short_cols,col_types = cols())
  }
  
}

left <- function (x,n) substr(x,1,n)
right <- function(x,n) substr(x,nchar(x)-n+1,nchar(x))

training <- read_csv('./DRF_Data/HaitiPixels.csv') %>% 
  mutate(Class = factor(Class)) %>% 
  mutate(Tarp = factor(ifelse(Class=='Blue Tarp','Tarp','Not Tarp'))) %>% 
  mutate(Tarp = fct_relevel(Tarp,c('Tarp','Not Tarp')))

holdout <- data.frame(path = list.files('./DRF_Data/holdouts',recursive = T,pattern = '.txt',full.names = T)) %>% 
  rowwise() %>%
  mutate(data = list(get_holdouts(path))) %>% 
  ungroup()
```

```{r}
str(training)
str(holdout)

```

The dataset comprises nine files: one training set (63,241 observations) and eight holdout files. Each training observation corresponds to a pixel, with four variables: classification (Blue Tarp, Rooftop, Soil, Various Non-Tarp, Vegetation) and RGB values. A binary "Tarp"/"Not Tarp" column was added for focused analysis. The holdout set includes RGB values labeled as B1, B2, B3 and six coordinate columns. Initial exploration identified a duplicate file (orthovnir067_ROI_Blue_Tarps_data.txt), which was removed, resulting in a final holdout set exceeding 2,000,000 data points. Current efforts prioritize verifying RGB column correspondence across files to ensure data integrity for subsequent modeling.

#### Check if any data is missing

```{r}
missing_count <- training %>%
  summarise_all(~sum(is.na(.))) %>%
  gather(key = "Variable", value = "Missing_Count") %>%
  arrange(desc(Missing_Count))

print(missing_count)
```

There is **No** missing data in any column in the given data set.

#### The training data set

```{r}
summary(training)
```

### **Class Distribution indicates severe Imbalance in training dataset**

-   **Blue Tarp**: 2,022 pixels (3.1%)

-   **Non-Tarp Classes**: 63,219 pixels (96.9%)

    -   Rooftop: 9,903 (15.2%)

    -   Soil: 20,566 (31.5%)

    -   Various Non-Tarp: 4,744 (7.3%)

    -   Vegetation: 26,006 (39.8%)

    **Key Issue**: Extreme class imbalance with tarps being only \~3% of data

```{r}
#| fig.width: 9
#| fig.height: 4
#| fig.cap: 'Classifications of the training set.'
cols <- c('Blue Tarp' = 'blue','Rooftop' = 'darkgrey','Soil'='yellow','Various Non-Tarp' = 'darkred','Vegetation'='green')

training %>% 
  ggplot(aes(x=Tarp, fill = Class)) +
  geom_bar(position = "stack")+
  scale_fill_manual(values=cols)+
  labs(title = "Classifications of the training set",
       x = "Tarp Status")+
  theme(plot.title = element_text(hjust = .5))
```

The chart clearly highlights the imbalance on different classes in the training data set.

#### RGB Parameter analysis

```{r}
training_rgb <- training %>% 
  select(Red,Green,Blue) %>% 
  mutate(
    RGB = rgb(Red, Green, Blue, maxColorValue = 255)
  )
```

```{r}

#| fig.width: 6
#| fig.height: 3
#| fig.cap: 'Histograms of RGB Values'

df_long <- training_rgb %>%
  select(Red,Green,Blue) %>% 
  pivot_longer(cols = c(Red, Green, Blue), names_to = "Channel", values_to = "Value")

# Create histograms for each channel
ggplot(df_long, aes(x = Value, fill = Channel)) +
  geom_histogram(binwidth = 5, alpha = 0.3, position = "identity") +
  facet_wrap(~ Channel, scales = "free_y") +
  labs(title = "Histograms of RGB Values", x = "Value", y = "Count") +
  scale_fill_manual(values = c("blue", "green", "red")) +
  theme_minimal()+
  theme(plot.title = element_text(hjust = .5))
```

The RGB histogram analysis reveals a vegetation-heavy dataset with bimodal distributions across all channels, indicating diverse lighting conditions and surface types. Red values are heavily skewed low (typical of vegetated areas), while green and blue show distinct peaks. Overlapping distributions across all channels make simple RGB thresholding ineffective for blue tarp detection. Success will require band ratios, vegetation indices, and multi-band feature combinations rather than single-channel classification approaches.

## The Holdout data set

```{r}
holdout_combo <- holdout %>% 
  unnest(data) %>% 
  na.omit()%>% 
  mutate(b123 = rgb(B1,B2,B3,maxColorValue = 255),
         b213 = rgb(B2,B1,B3,maxColorValue = 255),
         b132 = rgb(B1,B3,B2,maxColorValue = 255),
         b312 = rgb(B3,B1,B2,maxColorValue = 255),
         b321 = rgb(B3,B2,B1,maxColorValue = 255),
         b231 = rgb(B2,B3,B1,maxColorValue = 255)) %>% 
  rename(lon=Long,lat=Lat) %>% 
  select(path,lat,lon,B1,B2,B3,b123,b132,b213,b231,b321,b312) %>% 
  nest(data = c(lat,lon,B1,B2,B3,b123,b132,b213,b231,b321,b312))

all_holdout <- holdout_combo %>% 
  select(data) %>% 
  unnest(data) 
```

```{r}
summary(all_holdout)
```

#### Exploratory Data Analysis on a Training Data Subset

```{r}
library(tidyverse)
library(GGally)
library(ggthemes)
library(scales)
library(gridExtra)
library(knitr)
library(ggplot2)
```

```{r}
training %>%
  count(Class) %>%
  ggplot(aes(x = reorder(Class, -n), y = n, fill = Class)) +
  geom_col(show.legend = FALSE) +
  geom_text(aes(label = n), vjust = -0.5) +
  labs(title = "Class Distribution in Training Set", x = "Class", y = "Pixel Count") +
  theme_minimal()
```

-   bar chart shows how many samples we have for each class in the training data
-   helps visualize if dataset is balanced or not
-   vegetation and soil have the highest counts (26,006 and 20,566)
-   blue tarp has only 2,022 points — *very underrepresented*
-   clear *class imbalance* in training set
    -   important because blue tarp is the class we care about most
    -   high risk of model predicting majority classes more often (soil, vegetation)
    -   need to apply class balancing techniques (oversampling, undersampling, etc.)
    -   accuracy alone won’t be enough of a metric due to the class imbalance --\> *need precision, recall, and F1-score* especially for blue tarp detection

```{r}
training_long <- training %>%
  pivot_longer(cols = c(Red, Green, Blue), names_to = "Channel", values_to = "Value")

ggplot(training_long, aes(x = Value, fill = Class)) +
  geom_density(alpha = 0.6) +
  facet_wrap(~ Channel, scales = "free") +
  labs(title = "RGB Density Distributions by Class", x = "Pixel Value", y = "Density") +
  theme_minimal()
```

-   shows how each point's intensity (0–255) for RGB channels are distributed across each class
-   helps compare color profiles of different classes
-   blue tarp has *strong* peak in the blue channel around mid-range values, unlike other classes
-   vegetation and soil have higher green and red intensity ranges compared to blue tarp
    -   this makes sense (plants are generally gonna have more green than blue, etc.)
-   classes show overlapping but distinct density curves, especially for blue channel
-   supports idea that blue tarps can be distinguished based on RGB combos
    -   proves including raw RGB values in the model would be very informative/useful
-   could suggest *derived features (like blue-minus-red)* to further separate classes and to see if more patterns arise
-   but some RGB overlap between classes could cause classification errors

```{r}
training_long %>%
  ggplot(aes(x = Channel, y = Value, fill = Class)) +
  geom_boxplot(alpha = 0.7, outlier.shape = NA) +
  labs(title = "RGB Boxplots by Class", x = "Channel", y = "Pixel Value") +
  theme_minimal()
```

-   shows pixel value range and spread across RGB channels for each class
    -   blue tarp has higher blue values and lower red/green medians --\> matches what we expect
    -   vegetation shows low RGB values, especially in blue, most likely due to green --\> matches what we expect
    -   soil and rooftop have overlapping distributions, could be harder to separate (brown is usually a mix of all three)
    -   various non-tarp shows wide spread in all channels, which could be a mix of materials/colors
    -   confirms blue tarp has a distinct color profile
-   supports using RGB values as features
-   useful to detect overlapping classes that might need more advanced features to separate

```{r ggpairs-clean-final, fig.width=6, fig.height=6}
ggpairs(training, columns = c("Red", "Green", "Blue"), mapping = aes(color = Class)) +
  theme_bw()
```

-   shows pairwise relationships between red, green, + blue pixel values by class\
-   above the diagonal shows correlation coefficients between each RGB pair\
-   below the diagonal shows scatterplots of RGB combinations colored by class\
-   diagonal shows density plots for each channel, split by class
    -   blue tarp class has more spread in blue values, distinct from other classes\
    -   red and green values are highly correlated, especially for soil, rooftop, and vegetation\
-   most classes form clear linear patterns in scatterplots --\> suggests RGB channels are not independent\
-   classes overlap, but blue tarp shows slightly different distribution in blue-green and blue-red\
-   useful to spot multicollinearity + see if certain combos might help with class separation

```{r}
training <- training %>%
  mutate(
    BlueRatio = Blue / (Red + Green + Blue + 1),
    GreenMinusRed = Green - Red,
    BlueMinusRed = Blue - Red
  )

ggplot(training, aes(x = BlueRatio, fill = Class)) +
  geom_density(alpha = 0.6) +
  labs(title = "Blue Ratio Distribution by Class", x = "Blue Ratio", y = "Density") +
  theme_minimal()
```

-   shows distribution of a derived feature: blue ratio = blue / (red + green + blue + 1)\
-   helps compare how “blue-dominant” each class is relative to total pixel profile
    -   blue tarp has much higher blue ratio values than all other classes - matches what we expect
    -   other classes cluster at lower blue ratio values\
-   useful feature for separating blue tarp from non-tarp classes\
-   derived feature simplifies RGB profile into one ratio/value
-   density curves show low overlap between blue tarp + other classes — good at discriminating between blue tarp or not?

#### Exploratory Data Analysis on a Test Data Subset

```{r}
all_holdout <- all_holdout %>%
  rename(Red = B1, Green = B2, Blue = B3)  # adjust if your columns are named differently

all_holdout_long <- all_holdout %>%
  pivot_longer(cols = c(Red, Green, Blue), names_to = "Channel", values_to = "Value")

ggplot(all_holdout_long, aes(x = Value, fill = Channel)) +
  geom_histogram(bins = 30, alpha = 0.6, position = "identity") +
  facet_wrap(~ Channel, scales = "free") +
  labs(title = "Holdout RGB Histograms", x = "Pixel Value", y = "Frequency") +
  theme_minimal()
```

-   shows pixel value distributions in the holdout set
    -   blue skews low, with many pixels in lower intensity range\
    -   green peaks in the mid-range, tails off gradually\
    -   red more evenly spread, slight peaks near 250 (could be because of overexposure/white areas)
-   validates that holdout data has similar lighting + color as training set\
-   important for ensuring model generalizes well, consistent color patterns
-   confirms preprocessing applied correctly --\> RGB is within expected range (0–255)

```{r}
ggplot(all_holdout_long, aes(x = Value, fill = Channel)) +
  geom_density(alpha = 0.5) +
  facet_wrap(~ Channel, scales = "free") +
  labs(title = "Holdout RGB Density", x = "Pixel Value", y = "Density") +
  theme_minimal()
```

-   shows smoothed density distribution of red, green, and blue pixel values in holdout set\
-   similar shape to histograms, but easier to see underlying trends and smooth peaks\
-   blue values concentrated in lower range — many dark blue/gray pixels\
-   green peaks around 120–150, likely from vegetation or green surfaces\
-   red is more evenly distributed, with small peaks across the full range\
-   confirms holdout set has similar structure to training set RGB-wise\
-   smooth curves make it easier to spot mode shifts or color balance issues\
-   no extreme color outliers — suggests preprocessing is consistent\
-   useful for sanity-checking color distributions before applying model\
-   helps visually compare holdout vs training distributions for RGB inputs

```{r}
all_holdout <- all_holdout %>%
  mutate(
    BlueRatio = Blue / (Red + Green + Blue + 1),
    GreenMinusRed = Green - Red,
    BlueMinusRed = Blue - Red
  )

ggplot(all_holdout, aes(x = BlueRatio)) +
  geom_density(fill = "blue", alpha = 0.6) +
  labs(title = "Blue Ratio Distribution in Holdout Set", x = "Blue Ratio", y = "Density") +
  theme_minimal()
```

-   density plot showing distribution of BlueRatio in holdout (test) data\

-   BlueRatio = Blue / (Red + Green + Blue + 1) — emphasizes "blueness" in a pixel\

-   single, prominent peak around \~0.25 — most pixels not strongly blue-dominant\

-   smaller secondary bumps between 0.3–0.35 — may indicate some tarp or roof areas\

-   long right tail shows a few pixels with high blue dominance\

-   helps check if BlueRatio patterns in test set match training set\

-   no extreme outliers — BlueRatio remains bounded and stable\

-   important for generalizability — confirms that model inputs (like BlueRatio) behave similarly across both sets\

-   supports BlueRatio as a meaningful engineered feature for distinguishing classes (especially Blue Tarp)

    #### Exploratory Data Analysis on a Both Data Sets

```{r}
train_rgb <- training %>%
  select(Red, Green, Blue) %>%
  mutate(Source = "Training")

holdout_rgb <- all_holdout %>%
  select(Red, Green, Blue) %>%
  mutate(Source = "Holdout")

combined_rgb <- bind_rows(train_rgb, holdout_rgb) %>%
  pivot_longer(cols = c(Red, Green, Blue), names_to = "Channel", values_to = "Value")

ggplot(combined_rgb, aes(x = Value, fill = Source)) +
  geom_density(alpha = 0.5) +
  facet_wrap(~ Channel, scales = "free") +
  labs(title = "RGB Distribution Comparison: Training vs. Holdout", x = "Pixel Value", y = "Density") +
  theme_minimal()
```

-   compares Red, Green, and Blue pixel value distributions across training and holdout sets\
-   helps check for dataset drift or mismatch in RGB characteristics\
-   Blue and Green channels show high overlap — distributions are similar\
-   Red channel shows some divergence — especially at extreme values near 255\
-   holdout has fewer high-red pixels compared to training set\
-   overall, good alignment across sets — suggests preprocessing preserved RGB balance\
-   confirms model trained on this training data is likely to generalize well on holdout data\
-   visual inspection step that validates ML assumption: training and test data come from same distribution\
-   worth noting slight differences for channels — might affect class balance (e.g., rooftop/soil detection)

```{r}
train_features <- training %>%
  select(BlueRatio, GreenMinusRed, BlueMinusRed) %>%
  mutate(Source = "Training")

holdout_features <- all_holdout %>%
  select(BlueRatio, GreenMinusRed, BlueMinusRed) %>%
  mutate(Source = "Holdout")

combined_features <- bind_rows(train_features, holdout_features) %>%
  pivot_longer(cols = c(BlueRatio, GreenMinusRed, BlueMinusRed), 
               names_to = "Feature", values_to = "Value")

ggplot(combined_features, aes(x = Value, fill = Source)) +
  geom_density(alpha = 0.5) +
  facet_wrap(~ Feature, scales = "free") +
  labs(title = "Derived Feature Distribution Comparison", 
       x = "Value", y = "Density") +
  theme_minimal()
```

-   compares training vs. holdout distributions for engineered features: BlueMinusRed, BlueRatio, and GreenMinusRed\
-   verifies whether derived features behave similarly in both datasets — critical for model generalization\
-   BlueRatio shows strong alignment — reassuring since it's a key color-based signal for blue tarp detection\
-   BlueMinusRed and GreenMinusRed also largely match — slight spread differences but no major shift\
-   confirms derived features are stable across datasets\
-   supports our ML assumption that learned color ratios in training should work similarly on unseen data\
-   overall: good indication that these engineered features are reliable inputs for modeling

## Additional Visuals

```{r}
ggplot(training, aes(x = Red, y = Blue, color = Class)) +
  geom_point(alpha = 0.4) +
  labs(title = "Red vs Blue Color Space by Class") +
  theme_minimal()

ggplot(training, aes(x = Green, y = Blue, color = Class)) +
  geom_point(alpha = 0.4) +
  labs(title = "Green vs Blue Color Space by Class") +
  theme_minimal()

ggplot(training, aes(x = Red, y = Green, color = Class)) +
  geom_point(alpha = 0.4) +
  labs(title = "Red vs Green Color Space by Class") +
  theme_minimal()

```

-   Red vs Blue: clearly shows Blue Tarp pixels skew toward higher Blue values
-   Green vs Blue: Vegetation clusters lower in Blue
-   Red vs Green: Rooftop and Soil pixels mostly overlap, but Vegetation again shifts lower in Red
-   confirms multicollinearity exists (high RGB correlation), so derived features or PCA is probably needed

```{r}
library(corrplot)

cor_matrix <- cor(training %>% 
                    select(Red, Green, Blue, BlueRatio, GreenMinusRed, BlueMinusRed) %>% 
                    na.omit())

corrplot(cor_matrix, method = "color", addCoef.col = "black", tl.cex = 0.8)

```

-   visualizes how strongly each feature is correlated with others using Pearson correlation
-   RGB are very highly correlated (\>0.94) which is expected since they come from the same pixels
-   BlueRatio has low correlation with raw channels
-   GreenMinusRed + BlueMinusRed are negatively correlated with Red & Green, positively with Blue
-   BlueMinusRed + GreenMinusRed are strongly correlated with each other (0.77)
-   confirms BlueRatio + one color difference may be useful --\> less redundant for modeling

```{r}
training %>%
  pivot_longer(cols = c(Red, Green, Blue, BlueRatio, BlueMinusRed), names_to = "Feature", values_to = "Value") %>%
  ggplot(aes(x = Class, y = Value, fill = Class)) +
  geom_boxplot() +
  facet_wrap(~ Feature, scales = "free", ncol = 2) +
  labs(title = "Feature Distributions by Class") +
  theme_minimal()
```

-   shows distribution of each feature (RGB + derived) for every class
-   BlueRatio + BlueMinusRed separates Blue Tarp from all other classes well
-   Red and Green show broader spread and some class overlap
-   Vegetation stands out in Red + Blue channels with low values which is expected
-   also useful to spot outliers and skewed distributions within each class

```{r}
train_pca <- prcomp(training[, c("Red", "Green", "Blue")], center = TRUE, scale. = TRUE)
pca_df <- as_tibble(train_pca$x[, 1:2]) %>%
  bind_cols(Class = training$Class)

ggplot(pca_df, aes(x = PC1, y = PC2, color = Class)) +
  geom_point(alpha = 0.4) +
  labs(title = "PCA of RGB Features") +
  theme_minimal()

```

-   PCA reduces RGB data to 2D while preserving most variance
-   Blue Tarp + Vegetation form more distinct clusters
-   Soil, Rooftop, + Various Non-Tarp overlap more

## Data Preparation

-   B1 = Red (R)
-   B2 = Green (G)
-   B3 = Blue (B)

The training dataset was partitioned using an 80/20 split, allocating 80% of the data to the training set and the remaining 20% to the testing set. This proportion was selected due to the substantial size of the original dataset, which contains over sixty thousand observations. As a result, the testing set comprises more than ten thousand observations, providing a robust sample for model validation. Prioritizing a larger training set is expected to enhance model performance by allowing the algorithms to learn from a greater volume of data. Furthermore, since ten-fold cross-validation will be employed during model training, a larger training set ensures that each fold contains a sufficient number of observations. The data split was stratified based on the Tarp variable to maintain consistent proportions of ‘Blue Tarp’ and ‘Not Blue Tarp’ observations across both the training and testing sets

## Model Fitting, Tuning Parameter Selection, Evaluation

As outlined above, the full training set was split into training and testing sets by an 80/20 proportion. A recipe with a pre-processing step for synthetic minority oversampling then each model was created using the same recipe to ensure the models could be compared fairly. The logistic regression model was created using the `glm` engine and the LDA and QDA models were created using the `MASS` engine. The model workflows were then created.

```{r}
haiti_split = initial_split(training,prop = 0.8,strata = Tarp)
haiti_train = training(haiti_split)
haiti_test = testing(haiti_split)

```

Let's check the proportion of the split.

```{r}
original_props <- training %>%
  count(Tarp) %>%
  mutate(prop = n / sum(n)) %>%
  rename(original_n = n, original_prop = prop)

train_props <- haiti_train %>%
  count(Tarp) %>%
  mutate(prop = n / sum(n)) %>%
  rename(train_n = n, train_prop = prop)

test_props <- haiti_test %>%
  count(Tarp) %>%
  mutate(prop = n / sum(n)) %>%
  rename(test_n = n, test_prop = prop)

prop_comparison <- original_props %>%
  full_join(train_props, by = "Tarp") %>%
  full_join(test_props, by = "Tarp") %>%
  mutate(
    train_diff = abs(original_prop - train_prop),
    test_diff = abs(original_prop - test_prop)
  )

print(prop_comparison)
```

```{r}
formula <- Tarp ~ Red + Green + Blue
recipe <- recipe(formula,data=haiti_train) %>% 
  step_smote(Tarp,seed = 9,over_ratio = tune())
  #step_normalize(all_numeric_predictors()) # Probably can be replaced with SMOTE
```

#### Model Creation

```{r}
resamples <- vfold_cv(haiti_train, v=10, strata=Tarp)
custom_metrics <- metric_set(roc_auc, accuracy,f_meas)
cv_control <- control_resamples(save_pred=TRUE,save_workflow = T,event_level = 'first')

## Phase 1
logreg_spec <- logistic_reg(mode='classification', engine='glm')
lda_spec <- discrim_linear(mode = 'classification',engine = 'MASS')
qda_spec <- discrim_quad(mode = 'classification',engine = 'MASS')

# Phase 2 
# K-NEAREST NEIGHBORS (KNN)
knn_spec <- nearest_neighbor(
  neighbors = 5,           
  weight_func = "rectangular",  
  dist_power = 2              
) %>%
  set_mode("classification") %>%
  set_engine("kknn")

# PENALIZED LOGISTIC REGRESSION (ELASTIC NET)
elastic_net_spec <- logistic_reg(
  penalty = 0.01,           
  mixture = 0.5            
) %>%
  set_mode("classification") %>%
  set_engine("glmnet")


logreg_wf <- workflow() %>%
    add_recipe(recipe) %>%
    add_model(logreg_spec)

lda_wf <- workflow() %>% 
  add_recipe(recipe) %>% 
  add_model(lda_spec)

qda_wf <- workflow() %>% 
  add_recipe(recipe) %>% 
  add_model(qda_spec)

# KNN workflow
knn_wf <- workflow() %>%
  add_recipe(recipe) %>%
  add_model(knn_spec)

# Elastic Net workflow
elastic_net_wf <- workflow() %>%
  add_recipe(recipe) %>%
  add_model(elastic_net_spec)
```

This code creates a **hyperparameter tuning function specifically for SMOTE over-sampling ratios** and applies it to three LR, LDA, QDA classification models

**Problem Being Solved**:

-   Your earlier results showed **class imbalance** (high accuracy but lower F-measures)

-   This function finds the **optimal SMOTE oversampling ratio** for each model

-   Goal: Improve minority class detection (better F-measure) while maintaining ROC-AUC

**Expected Outcome**:

-   Each model will be tuned to find the best balance between over-sampling and model performance

-   The `lr_results$best`, `lda_results$best`, and `qda_results$best` will contain optimal over-sampling ratios

-   Plots will show how performance varies with different oversampling levels

    **Key Insight**: This is a sophisticated approach to handle class imbalance by **automatically finding the optimal amount of synthetic minority samples** to generate for each classification algorithm!

```{r}

tune_ratio <- function (wf, best_by, modelname, size = 20) {
  parameters <- extract_parameter_set_dials(wf) %>%
    update(over_ratio = over_ratio(c(0,1))
           )
  resamples <- vfold_cv(haiti_train, v=10, strata=Tarp)
  custom_metrics <- metric_set(roc_auc,f_meas)
  cv_control <- control_resamples(save_pred=TRUE,save_workflow = T,event_level = 'first')

  tune_results <- tune_grid(wf,
      resamples=resamples,
      control=cv_control,
      grid=grid_regular(parameters, levels=size),
      metrics = custom_metrics)

  best <- select_best(tune_results,metric = best_by)
  plot <- autoplot(tune_results) + ggtitle(modelname)
  
  return(list(best=best,plot=plot))
}

lr_results <- tune_ratio(logreg_wf,best_by = 'f_meas', modelname = "Logistic Regression.")
lda_results <- tune_ratio(lda_wf,best_by = 'f_meas', modelname ='LDA')
qda_results <- tune_ratio(qda_wf,best_by = 'f_meas',modelname = 'QDA')

```

```{r}
cat("Fitting KNN...\n")
knn_results <- tune_ratio(knn_wf,best_by = 'f_meas', modelname = "KNN.")


#elastic_net_results <- fit_resamples(elastic_net_wf, resamples, metrics = custom_metrics, control = cv_control)

```

```{r}
elastic_net_results <- tune_ratio(elastic_net_wf,best_by = 'f_meas', modelname = "Elastic Net")
cat("Fitting Elastic Net...\n")
```

```{r}
lr_results$plot + lda_results$plot + qda_results$plot

```

```{r}
knn_results$plot
```

```{r}
elastic_net_results$plot
```

By ROC-AUC:

-    QDA: \~0.99835 (best overall)

-    Logistic Regression: \~0.99876

-    LDA: \~0.9940

-   KNN: \~0.99315

-   Elastic Net: \~0.9925

By F-Measure :

-   KNN: \~0.956 (best F-measure)

-   QDA: \~0.91 (at optimal ratio)

-   Logistic Regression: \~0.84 (declining trend)

-   LDA: \~0.766 (improving trend)

-   Elastic Net: \~0.75 (plateaus quickly)

1.  QDA shows excellent ROC-AUC but declining F-measure with more over-sampling

2.   KNN maintains high F-measure across most ratios

3.  Elastic Net shows rapid improvement then plateaus

4.  Logistic Regression shows declining performance with more over-sampling

```{r}
knn_spec <- nearest_neighbor(
  neighbors = tune(),           
  weight_func = "rectangular",  
  dist_power = 2              
) %>%
  set_mode("classification") %>%
  set_engine("kknn")

# PENALIZED LOGISTIC REGRESSION (ELASTIC NET)
elastic_net_spec <- logistic_reg(
  penalty = 0.01,           
  mixture = 0.5            
) %>%
  set_mode("classification") %>%
  set_engine("glmnet")
```

Finally, the models were finalized using the optimal over-sampling ratio, and then trained using ten-fold cross validation, the results are summarized in the table below.

```{r}
logreg_wf <- logreg_wf %>% 
  finalize_workflow(parameters = lr_results$best)
lda_wf <- lda_wf %>% 
  finalize_workflow(parameters = lda_results$best)
qda_wf <- qda_wf %>% 
  finalize_workflow(parameters = qda_results$best)
knn_wf <- knn_wf %>% 
  finalize_workflow(parameters = knn_results$best)
elastic_net_wf <- elastic_net_wf %>% 
  finalize_workflow(parameters = elastic_net_results$best)
```

```{r}
logreg_cv <- fit_resamples(logreg_wf,resamples,metrics = custom_metrics,control = cv_control)
lda_cv <- fit_resamples(lda_wf,resamples,metrics=custom_metrics,control=cv_control)
qda_cv <- fit_resamples(qda_wf,resamples,metrics=custom_metrics,control=cv_control)
knn_cv <- fit_resamples(knn_wf,resamples,metrics=custom_metrics,control=cv_control)
elasticnet_cv <- fit_resamples(elastic_net_wf,resamples,metrics=custom_metrics,control=cv_control)

cv_metrics <- bind_rows(
  collect_metrics(logreg_cv) %>% mutate(model='Logistic Regression'),
  collect_metrics(lda_cv) %>% mutate(model='LDA'),
  collect_metrics(qda_cv) %>% mutate(model='QDA'),
  collect_metrics(knn_cv) %>% mutate(model='KNN'),
  collect_metrics(elasticnet_cv) %>% mutate(model='ElasticNet')
) %>% 
  select(-.estimator,-n,-.config) %>% 
  pivot_wider(names_from = .metric,
              values_from = c(mean,std_err))
```

```{r}

cv_metrics <- bind_rows(
collect_metrics(lda_cv) %>%
  mutate(model="LDA"),
collect_metrics(qda_cv) %>%
  mutate(model="QDA"),
collect_metrics(logreg_cv) %>%
  mutate(model="Logisitic Regression"),
collect_metrics(knn_cv) %>%
  mutate(model="KNN"),
collect_metrics(elasticnet_cv) %>%
  mutate(model="ElasticNet")
)
cv_metrics %>%
select(-.estimator,-n,-.config) %>% 
pivot_wider(names_from = .metric,values_from = c(mean,std_err)) %>%  
knitr::kable(caption="Cross-validation performance metrics", digits=3)
```

**Excellent Model Performance Across All Methods**:

-   All five models demonstrate exceptionally high performance with accuracy scores between 98.4% and 99.5%

-   ROC-AUC scores ranging from 0.989 to 0.998 indicate excellent discriminative ability

## **Model-by-Model Analysis**

**Best Model: Logistic Regression**

-   99.9% ROC-AUC, 99.6% Accuracy, 93.6% F-Measure

**2nd Best Model: QDA & KNN**

-   99.8% ROC-AUC, 99.5% Accuracy, 91.8% F-Measure

**LDA**

-   99.4% ROC-AUC, 98.3% Accuracy, 76.6% F-Measure

**Insignificant Model : ElasticNet ( poor performance)**

-   99.1% ROC-AUC, 98.5% Accuracy, 77.6% F-Measure

## **Key Insights**

**Optimal Choice**: **Logistic Regression**

-   Best minority class detection

-   Highest overall accuracy

-   Most consistent performance

**Avoid**: **LDA & ELastic Net**

**Bottom Line**: SMOTE tuning successfully optimized the balance between classes, with Logistic Regression has better overall performance and minority class detection.

**Model Selection**: **Logistic Regression** appears to be the optimal choice, offering the best balance across all performance dimensions.

```{r}
ggplot(cv_metrics, aes(x=mean, y=model, xmin=mean - std_err, xmax=mean + std_err)) +
geom_point() +
geom_linerange() +
facet_wrap(~ .metric)
```

```{r}
roc_cv_plot <- function(model_cv, model_name) {
cv_predictions <- collect_predictions(model_cv)
cv_roc <- cv_predictions %>%
roc_curve(truth=Tarp, .pred_Tarp, event_level="first")
g <- autoplot(cv_roc) +
labs(title=model_name)
return(g)
}

g1 <- roc_cv_plot(logreg_cv, "Logistic regression")
g2 <- roc_cv_plot(lda_cv, "LDA")
g3 <- roc_cv_plot(qda_cv, "QDA")
g4 <- roc_cv_plot(knn_cv, "KNN")
g5 <- roc_cv_plot(elasticnet_cv, "Elastic Net")
g1 + g2 + g3
g4 + g5
```

Overlay Plot

```{r}
bind_rows(
collect_predictions(logreg_cv) %>% mutate(model="Logistic regression"),
collect_predictions(lda_cv) %>% mutate(model="LDA"),
collect_predictions(qda_cv) %>% mutate(model="QDA"),
collect_predictions(knn_cv) %>% mutate(model="KNN"),
collect_predictions(elasticnet_cv) %>% mutate(model="Elastic Net")
) %>%
group_by(model) %>%
roc_curve(truth=Tarp, .pred_Tarp, event_level="first") %>%
autoplot()
```

#### Threshold Selection

```{r}
threshold_scan_cv <- function(model_cv, data, model_name) {
    threshold_data <- model_cv %>% 
      fit_best() %>% 
      augment(data) %>% 
        probably::threshold_perf(truth = Tarp, estimate = .pred_Tarp,
            thresholds=seq(0.05, 0.95, 0.01), event_level="first",
            metrics=metric_set(f_meas))
    opt_threshold <- threshold_data %>%
        arrange(-.estimate) %>%
        first()
    g <- ggplot(threshold_data, aes(x=.threshold, y=.estimate)) +
        geom_line() +
        geom_point(data=opt_threshold, color="red", size=2) +
        labs(title=model_name, x="Threshold", y="F Measurement") +
        coord_cartesian(ylim=c(0.01, 0.9))
    return(list(
        graph=g,
        threshold=opt_threshold %>%
            pull(.threshold)
    ))
}

g1 <- threshold_scan_cv(logreg_cv, haiti_train, "Logistic regression")
g2 <- threshold_scan_cv(lda_cv, haiti_train, "LDA")
g3 <- threshold_scan_cv(qda_cv, haiti_train, "QDA")
g4 <- threshold_scan_cv(knn_cv, haiti_train, "KNN")
g5 <- threshold_scan_cv(elasticnet_cv, haiti_train, "Elastic Net")

traingraphs <- (g1$graph + g2$graph + g3$graph + g4$graph+ g5$graph) + plot_annotation(tag_level = 'new')
traingraphs+ plot_annotation(tag_levels = 'A')
```

Let's perform this on Test Data

```{r}
g1 <- threshold_scan_cv(logreg_cv, haiti_test, "Logistic regression")
g2 <- threshold_scan_cv(lda_cv, haiti_test, "LDA")
g3 <- threshold_scan_cv(qda_cv, haiti_test, "QDA")
g4 <- threshold_scan_cv(knn_cv, haiti_test, "KNN")
g5 <- threshold_scan_cv(elasticnet_cv, haiti_test, "Elastic Net")

lr_thresh <- g1$threshold
lda_thresh <- g2$threshold
qda_thresh <- g3$threshold
knn_thresh <- g4$threshold
elasticnet_thresh <- g5$threshold

testgraphs <- g1$graph + g2$graph + g3$graph + g4$graph + g5$graph + plot_annotation(tag_level = 'new')
testgraphs+ plot_annotation(tag_levels = 'A')
```

Test data confirms Logistic Regression as the best choice - it offers better performance (F-Measure = 0.85) with operational robustness that QDA lacks despite its slightly higher peak performance.

Let's do prediction on the test data set.

```{r}
logreg_pred <- logreg_cv %>% fit_best() %>%  augment(haiti_test)
lda_pred <- lda_cv %>% fit_best() %>%  augment(haiti_test)
qda_pred <- qda_cv %>% fit_best() %>%  augment(haiti_test)
knn_pred <- knn_cv %>% fit_best() %>%  augment(haiti_test)
elasticnet_pred <- elasticnet_cv %>% fit_best() %>%  augment(haiti_test)
```

```{r}
roctest <- bind_rows(
    roc_curve(logreg_pred,truth=Tarp, .pred_Tarp, event_level="first") %>% mutate(model="Logistic regression"),
    roc_curve(lda_pred,truth=Tarp, .pred_Tarp, event_level="first") %>% mutate(model="LDA"),
    roc_curve(qda_pred,truth=Tarp, .pred_Tarp, event_level="first") %>% mutate(model="QDA"),
    roc_curve(knn_pred,truth=Tarp, .pred_Tarp, event_level="first") %>% mutate(model="KNN"),
    roc_curve(elasticnet_pred,truth=Tarp, .pred_Tarp, event_level="first") %>% mutate(model="Elastic Net")
) %>%
ggplot(aes(x=1 - specificity, y=sensitivity, color=model)) +
    geom_line()+
    geom_abline(linetype = 'dashed')+
  labs(title='Testing')

roctest
```

```{r}
predict_at_threshold <- function(model, data, threshold) {
    return(
        model %>%
            augment(data) %>%
            mutate(.pred_class = probably::make_two_class_pred(.pred_Tarp,
                    c("Tarp", "Not Tarp"), threshold=threshold)
            )
    )
}

lr_test_pred <- predict_at_threshold(logreg_cv %>% fit_best(),haiti_test,lr_thresh)
lda_test_pred <- predict_at_threshold(lda_cv %>% fit_best(),haiti_test,lda_thresh)
qda_test_pred <- predict_at_threshold(qda_cv %>% fit_best(),haiti_test,qda_thresh)
knn_test_pred <- predict_at_threshold(knn_cv %>% fit_best(),haiti_test,knn_thresh)
elasticnet_test_pred <- predict_at_threshold(elasticnet_cv %>% fit_best(),haiti_test,elasticnet_thresh)
```

```{r}
cat("Confusion Matrix \n")
cat("\n Logisitic Regrssion \n")

lr_test_pred %>% conf_mat(truth = Tarp,estimate = .pred_class)
cat("\n LDA \n")

lda_test_pred %>% conf_mat(truth = Tarp,estimate = .pred_class)

cat("\n QDA \n")
qda_test_pred %>% conf_mat(truth = Tarp,estimate = .pred_class)

cat("\n KNN \n")
knn_test_pred %>% conf_mat(truth = Tarp,estimate = .pred_class)

cat("\n ELastic Net \n")
elasticnet_test_pred %>% conf_mat(truth = Tarp,estimate = .pred_class)
```

#### Logistic Regression provides the optimal balance of catching true Tarp cases while minimizing false alarms

##### Now the model is tuned and Logisitic Regression shows promising performance, let's test this on Holdout data set.

```{r}
all_holdout <- holdout_combo %>% 
  unnest(data) %>% 
  rename(Red=B1,Green=B2,Blue=B3)
holdout_pred <- predict_at_threshold(logreg_cv %>% fit_best(),all_holdout,lr_thresh)



total.found <- holdout_pred %>% 
  filter(.pred_class=='Tarp') %>% 
  select(.pred_Tarp,lat,lon,Red,Green,Blue) %>% 
  nrow()

holdout_pred <- holdout_pred %>% 
  select(.pred_class,.pred_Tarp,`.pred_Not Tarp`,path,lat,lon,Red,Green,Blue) %>% 
  nest(data = c(.pred_class,.pred_Tarp,`.pred_Not Tarp`,lat,lon,Red,Green,Blue))
```

```{r}
holdout_analysis <- holdout_pred %>%
  unnest(data)
```

```{r}

prediction_summary <- holdout_analysis %>%
  group_by(.pred_class) %>%
  summarise(
    count = n(),
    percentage = n() / nrow(holdout_analysis) * 100,
    mean_prob_tarp = mean(.pred_Tarp),
    median_prob_tarp = median(.pred_Tarp),
    sd_prob_tarp = sd(.pred_Tarp),
    mean_red = mean(Red),
    mean_green = mean(Green), 
    mean_blue = mean(Blue),
    .groups = 'drop'
  )

cat("\n=== PREDICTION SUMMARY ===\n")
print(prediction_summary)

```

```{r}

total_pixels <- sum(prediction_summary$count)
cat("\nTotal pixels analyzed:", comma(total_pixels), "\n")
cat("Tarp detections:", comma(prediction_summary$count[1]), "\n")
cat("Detection rate:", round(prediction_summary$percentage[1], 2), "%\n")
```

```{r}
rgb_by_prediction <- holdout_analysis %>%
  group_by(.pred_class) %>%
  summarise(
    count = n(),
    mean_red = mean(Red),
    mean_green = mean(Green),
    mean_blue = mean(Blue),
    sd_red = sd(Red),
    sd_green = sd(Green),
    sd_blue = sd(Blue),
    .groups = 'drop'
  )

cat("RGB Characteristics by Prediction:\n")
print(rgb_by_prediction)
```

```{r}
cat("\n=== PREDICTION CONFIDENCE ANALYSIS ===\n")

confidence_summary <- holdout_analysis %>%
  summarise(
    mean_tarp_prob = mean(.pred_Tarp),
    median_tarp_prob = median(.pred_Tarp),
    sd_tarp_prob = sd(.pred_Tarp),
    min_tarp_prob = min(.pred_Tarp),
    max_tarp_prob = max(.pred_Tarp),
    q25_tarp_prob = quantile(.pred_Tarp, 0.25),
    q75_tarp_prob = quantile(.pred_Tarp, 0.75)
  )

cat("Tarp Probability Distribution:\n")
print(confidence_summary)
```

### **Model Behavior**

-   **Conservative classifier**: Model assigns low Tarp probabilities to most pixels

-   **Sparse detections**: Only a small fraction of pixels exceeded threshold

-   **High precision expected**: When model predicts Tarp, it's likely correct

```{r}
confidence_by_class <- holdout_analysis %>%
  group_by(.pred_class) %>%
  summarise(
    count = n(),
    percentage = n() / nrow(holdout_analysis) * 100,
    mean_prob = mean(.pred_Tarp),
    median_prob = median(.pred_Tarp),
    sd_prob = sd(.pred_Tarp),
    min_prob = min(.pred_Tarp),
    max_prob = max(.pred_Tarp),
    .groups = 'drop'
  )

cat("\nConfidence by Predicted Class:\n")
print(confidence_by_class)
```

**Detection Performance:**

-   **4.6% detection rate** - Excellent balance (not too high/low)

-   **91,764 tarp detections** out of 2+ million pixels

-   **Strong generalization** to new geographic areas

**Confidence Quality:**

-   **73% mean confidence** for tarp predictions (good discrimination)

-   **98% mean confidence** for non-tarp predictions (excellent certainty)
