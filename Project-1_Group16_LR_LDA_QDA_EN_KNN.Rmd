---
title:  |
        | Disaster Relief Project
        | Part One
author: |
        | Group 16
        | Srivatsa Balasubramanyam, Kaya Oguz, Kristina Quintana, Shriya Kuruba

date: June 23, 2025
output:
  bookdown::html_document2:
    number_sections: true
    toc: false
    extra_dependencies: ["float"]
subtitle: |
          | University of Virginia
          | School of Data Science
          | Statistical Learning
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=TRUE, cache=TRUE, autodep=TRUE, fig.align="center")
```

### Load the required library

```{r message=FALSE, warning=FALSE}
rm(list=ls())
library(tidymodels)
library(tidyverse)
library(patchwork)
library(ggplot2)
library(dplyr)
library(themis)
library(discrim)
```

```{r}
#| cache: FALSE
#| message: false
library(doParallel)

cl <- makePSOCKcluster(parallelly::availableCores(omit = 4))
registerDoParallel(cl)
```

**Introduction**

In the immediate aftermath of the January 12, 2010 earthquake in Haiti, nations worldwide responded to the Haitian government’s appeal for aid by pledging funds and dispatching rescue personnel. However, as rescue operations commenced, a critical challenge emerged: the destruction of communications infrastructure made it extremely difficult to determine where to direct emergency supplies and assistance.

**Challenge: Locating Displaced Populations**

With millions displaced and traditional communication channels inoperable, rescue workers needed an effective method to identify the locations of those most in need. It was observed that many displaced individuals were constructing makeshift shelters using blue tarps, which became a key visual indicator of temporary settlements

A team from the Rochester Institute of Technology addressed this challenge by collecting high-resolution, geo-referenced aerial photographs over the affected areas. These images, while rich in detail, represented an enormous volume of data, making manual analysis infeasible given the urgency and scale of the crisis.

**Solution: Statistical Modeling of Aerial Imagery**

To overcome the limitations of manual image analysis, statistical modeling techniques were employed to automate the identification of makeshift shelters. The process involved:

-   Using the color characteristics of blue tarps as the primary indicator of shelter locations.
-   Extracting pixel-level data (Red, Green, Blue values) from the aerial images as predictor variables.
-   Applying supervised learning algorithms to classify image regions as either containing blue tarps (shelters) or not.

**Model Selection and Evaluation**

Many statistical models will be considered for this task:

Models without tuning parameters – Logistic Regression – LDA (Linear Discriminant Analysis) – QDA (Quadratic Discriminant Analysis)

• Models with tuning parameters – KNN (K-nearest neighbor) – Penalized Logistic Regression (elastic net penalty) – Ensemble method: random forest (ranger) or boosting (XGBoost) – Support Vector Machines (SVM) ∗ linear kernel ∗ polynomial kernel ∗ radial basis function kernel

#### Load the Haiti Dataset

```{r message=FALSE, warning=FALSE}
get_holdouts <- function(file) {
  
  infile <- read_lines(file,n_max = 20)
  
  col_row <- infile %>% 
    data.frame() %>% 
    mutate(text = gsub(';','',.)) %>% 
    select(-.)%>% 
    mutate(end = right(text,2) == 'B3',
           row = row_number()) %>% 
    filter(end==T) %>% 
    select(row)
  
  f <- suppressWarnings(read_table(file,skip = col_row$row-1,n_max = 20,col_types = cols()))
  
  long_cols <- c('ID','X','Y','Map_X','Map_Y','Lat','Long','B1','B2','B3')
  short_cols <- c('B1','B2','B3')
  
  if (ncol(f) > 4) {
    final <- read_table(file,skip = col_row$row,col_names = long_cols,col_types = cols())
  } else {
    final <- read_table(file,skip = col_row$row,col_names = short_cols,col_types = cols())
  }
  
}

left <- function (x,n) substr(x,1,n)
right <- function(x,n) substr(x,nchar(x)-n+1,nchar(x))

training <- read_csv('./DRF_Data/HaitiPixels.csv') %>% 
  mutate(Class = factor(Class)) %>% 
  mutate(Tarp = factor(ifelse(Class=='Blue Tarp','Tarp','Not Tarp'))) %>% 
  mutate(Tarp = fct_relevel(Tarp,c('Tarp','Not Tarp')))

holdout <- data.frame(path = list.files('./DRF_Data/holdouts',recursive = T,pattern = '.txt',full.names = T)) %>% 
  rowwise() %>%
  mutate(data = list(get_holdouts(path))) %>% 
  ungroup()
```

```{r}
str(training)
str(holdout)

```

The dataset comprises nine files: one training set (63,241 observations) and eight holdout files. Each training observation corresponds to a pixel, with four variables: classification (Blue Tarp, Rooftop, Soil, Various Non-Tarp, Vegetation) and RGB values. A binary "Tarp"/"Not Tarp" column was added for focused analysis. The holdout set includes RGB values labeled as B1, B2, B3 and six coordinate columns. Initial exploration identified a duplicate file (orthovnir067_ROI_Blue_Tarps_data.txt), which was removed, resulting in a final holdout set exceeding 2,000,000 data points. Current efforts prioritize verifying RGB column correspondence across files to ensure data integrity for subsequent modeling.

#### Check if any data is missing

```{r}
missing_count <- training %>%
  summarise_all(~sum(is.na(.))) %>%
  gather(key = "Variable", value = "Missing_Count") %>%
  arrange(desc(Missing_Count))

print(missing_count)
```

There is **No** missing data in any column in the given data set.

#### The training data set

```{r}
summary(training)
```

### **Class Distribution indicates severe Imbalance in training dataset**

-   **Blue Tarp**: 2,022 pixels (3.1%)

-   **Non-Tarp Classes**: 63,219 pixels (96.9%)

    -   Rooftop: 9,903 (15.2%)

    -   Soil: 20,566 (31.5%)

    -   Various Non-Tarp: 4,744 (7.3%)

    -   Vegetation: 26,006 (39.8%)

    **Key Issue**: Extreme class imbalance with tarps being only \~3% of data

```{r}
#| fig.width: 9
#| fig.height: 4
#| fig.cap: 'Classifications of the training set.'
cols <- c('Blue Tarp' = 'blue','Rooftop' = 'darkgrey','Soil'='yellow','Various Non-Tarp' = 'darkred','Vegetation'='green')

training %>% 
  ggplot(aes(x=Tarp, fill = Class)) +
  geom_bar(position = "stack")+
  scale_fill_manual(values=cols)+
  labs(title = "Classifications of the training set",
       x = "Tarp Status")+
  theme(plot.title = element_text(hjust = .5))
```

The chart clearly highlights the imbalance on different classes in the training data set.

#### RGB Parameter analysis

```{r}
training_rgb <- training %>% 
  select(Red,Green,Blue) %>% 
  mutate(
    RGB = rgb(Red, Green, Blue, maxColorValue = 255)
  )
```

```{r}
library(dplyr)
library(stringr)

all_holdout <- holdout %>%
  # 1) unnest the pixel data, keeping `path`
  unnest(data) %>%
  
  # 2) rename bands
  rename(
    Red   = B1,
    Green = B2,
    Blue  = B3
  ) %>%
  
  # 3) compute the same engineered features
  mutate(
    BlueRatio     = Blue / (Red + Green + Blue),
    GreenMinusRed = Green - Red,
    BlueMinusRed  = Blue  - Red
  ) %>%
  
  # 4) derive the true label from the filename in `path`
  mutate(
    Tarp = case_when(
      # Negative if filename ends in NOT or NON 
      str_detect(path, "(?:NOT|NON)_Blue_Tarps\\.txt$") ~ "no",
      # Positive if filename contains Blue_Tarps or Blue_Tarps_data
      str_detect(path, "Blue_Tarps(?:_data)?\\.txt$")       ~ "yes",
      TRUE ~ NA_character_
    ),
    Tarp = factor(Tarp, levels = c("no","yes"))
  ) %>%
  
  # 5) keep only the columns your RF needs + the true label
  select(
    Red, Green, Blue,
    BlueRatio, GreenMinusRed, BlueMinusRed,
    Tarp
  )

# Sanity check:
all_holdout %>% 
  summarise(
    total = n(),
    positives = sum(Tarp=="yes", na.rm=TRUE),
    negatives = sum(Tarp=="no",  na.rm=TRUE)
  )
```
```{r}
str(training)
```

```{r}
str(all_holdout)

```

```{r}

#| fig.width: 6
#| fig.height: 3
#| fig.cap: 'Histograms of RGB Values'

df_long <- training_rgb %>%
  select(Red,Green,Blue) %>% 
  pivot_longer(cols = c(Red, Green, Blue), names_to = "Channel", values_to = "Value")

# Create histograms for each channel
ggplot(df_long, aes(x = Value, fill = Channel)) +
  geom_histogram(binwidth = 5, alpha = 0.3, position = "identity") +
  facet_wrap(~ Channel, scales = "free_y") +
  labs(title = "Histograms of RGB Values", x = "Value", y = "Count") +
  scale_fill_manual(values = c("blue", "green", "red")) +
  theme_minimal()+
  theme(plot.title = element_text(hjust = .5))
```

The RGB histogram analysis reveals a vegetation-heavy dataset with bimodal distributions across all channels, indicating diverse lighting conditions and surface types. Red values are heavily skewed low (typical of vegetated areas), while green and blue show distinct peaks. Overlapping distributions across all channels make simple RGB thresholding ineffective for blue tarp detection. Success will require band ratios, vegetation indices, and multi-band feature combinations rather than single-channel classification approaches.

## The Holdout data set

```{r}
holdout_combo <- holdout %>% 
  unnest(data) %>% 
  na.omit()%>% 
  mutate(b123 = rgb(B1,B2,B3,maxColorValue = 255),
         b213 = rgb(B2,B1,B3,maxColorValue = 255),
         b132 = rgb(B1,B3,B2,maxColorValue = 255),
         b312 = rgb(B3,B1,B2,maxColorValue = 255),
         b321 = rgb(B3,B2,B1,maxColorValue = 255),
         b231 = rgb(B2,B3,B1,maxColorValue = 255)) %>% 
  rename(lon=Long,lat=Lat) %>% 
  select(path,lat,lon,B1,B2,B3,b123,b132,b213,b231,b321,b312) %>% 
  nest(data = c(lat,lon,B1,B2,B3,b123,b132,b213,b231,b321,b312))

all_holdout_1 <- holdout_combo %>% 
  select(data) %>% 
  unnest(data) 
```

```{r}
str(all_holdout_1)
```

#### Exploratory Data Analysis on a Training Data Subset

```{r}
library(tidyverse)
library(GGally)
library(ggthemes)
library(scales)
library(gridExtra)
library(knitr)
library(ggplot2)
```

```{r}
training %>%
  count(Class) %>%
  ggplot(aes(x = reorder(Class, -n), y = n, fill = Class)) +
  geom_col(show.legend = FALSE) +
  geom_text(aes(label = n), vjust = -0.5) +
  labs(title = "Class Distribution in Training Set", x = "Class", y = "Pixel Count") +
  theme_minimal()
```

-   bar chart shows how many samples we have for each class in the training data
-   helps visualize if dataset is balanced or not
-   vegetation and soil have the highest counts (26,006 and 20,566)
-   blue tarp has only 2,022 points — *very underrepresented*
-   clear *class imbalance* in training set
    -   important because blue tarp is the class we care about most
    -   high risk of model predicting majority classes more often (soil, vegetation)
    -   need to apply class balancing techniques (oversampling, undersampling, etc.)
    -   accuracy alone won’t be enough of a metric due to the class imbalance --\> *need precision, recall, and F1-score* especially for blue tarp detection

```{r}
training_long <- training %>%
  pivot_longer(cols = c(Red, Green, Blue), names_to = "Channel", values_to = "Value")

ggplot(training_long, aes(x = Value, fill = Class)) +
  geom_density(alpha = 0.6) +
  facet_wrap(~ Channel, scales = "free") +
  labs(title = "RGB Density Distributions by Class", x = "Pixel Value", y = "Density") +
  theme_minimal()
```

-   shows how each point's intensity (0–255) for RGB channels are distributed across each class
-   helps compare color profiles of different classes
-   blue tarp has *strong* peak in the blue channel around mid-range values, unlike other classes
-   vegetation and soil have higher green and red intensity ranges compared to blue tarp
    -   this makes sense (plants are generally gonna have more green than blue, etc.)
-   classes show overlapping but distinct density curves, especially for blue channel
-   supports idea that blue tarps can be distinguished based on RGB combos
    -   proves including raw RGB values in the model would be very informative/useful
-   could suggest *derived features (like blue-minus-red)* to further separate classes and to see if more patterns arise
-   but some RGB overlap between classes could cause classification errors

```{r}
training_long %>%
  ggplot(aes(x = Channel, y = Value, fill = Class)) +
  geom_boxplot(alpha = 0.7, outlier.shape = NA) +
  labs(title = "RGB Boxplots by Class", x = "Channel", y = "Pixel Value") +
  theme_minimal()
```

-   shows pixel value range and spread across RGB channels for each class
    -   blue tarp has higher blue values and lower red/green medians --\> matches what we expect
    -   vegetation shows low RGB values, especially in blue, most likely due to green --\> matches what we expect
    -   soil and rooftop have overlapping distributions, could be harder to separate (brown is usually a mix of all three)
    -   various non-tarp shows wide spread in all channels, which could be a mix of materials/colors
    -   confirms blue tarp has a distinct color profile
-   supports using RGB values as features
-   useful to detect overlapping classes that might need more advanced features to separate

```{r ggpairs-clean-final, fig.width=6, fig.height=6}
ggpairs(training, columns = c("Red", "Green", "Blue"), mapping = aes(color = Class)) +
  theme_bw()
```

-   shows pairwise relationships between red, green, + blue pixel values by class\
-   above the diagonal shows correlation coefficients between each RGB pair\
-   below the diagonal shows scatterplots of RGB combinations colored by class\
-   diagonal shows density plots for each channel, split by class
    -   blue tarp class has more spread in blue values, distinct from other classes\
    -   red and green values are highly correlated, especially for soil, rooftop, and vegetation\
-   most classes form clear linear patterns in scatterplots --\> suggests RGB channels are not independent\
-   classes overlap, but blue tarp shows slightly different distribution in blue-green and blue-red\
-   useful to spot multicollinearity + see if certain combos might help with class separation

```{r}
training <- training %>%
  mutate(
    BlueRatio = Blue / (Red + Green + Blue + 1),
    GreenMinusRed = Green - Red,
    BlueMinusRed = Blue - Red
  )

ggplot(training, aes(x = BlueRatio, fill = Class)) +
  geom_density(alpha = 0.6) +
  labs(title = "Blue Ratio Distribution by Class", x = "Blue Ratio", y = "Density") +
  theme_minimal()
```

-   shows distribution of a derived feature: blue ratio = blue / (red + green + blue + 1)\
-   helps compare how “blue-dominant” each class is relative to total pixel profile
    -   blue tarp has much higher blue ratio values than all other classes - matches what we expect
    -   other classes cluster at lower blue ratio values\
-   useful feature for separating blue tarp from non-tarp classes\
-   derived feature simplifies RGB profile into one ratio/value
-   density curves show low overlap between blue tarp + other classes — good at discriminating between blue tarp or not?

#### Exploratory Data Analysis on a Test Data Subset

```{r}
all_holdout_1 <- all_holdout_1 %>%
  rename(Red = B1, Green = B2, Blue = B3)  # adjust if your columns are named differently

all_holdout_long <- all_holdout_1 %>%
  pivot_longer(cols = c(Red, Green, Blue), names_to = "Channel", values_to = "Value")

ggplot(all_holdout_long, aes(x = Value, fill = Channel)) +
  geom_histogram(bins = 30, alpha = 0.6, position = "identity") +
  facet_wrap(~ Channel, scales = "free") +
  labs(title = "Holdout RGB Histograms", x = "Pixel Value", y = "Frequency") +
  theme_minimal()
```

-   shows pixel value distributions in the holdout set
    -   blue skews low, with many pixels in lower intensity range\
    -   green peaks in the mid-range, tails off gradually\
    -   red more evenly spread, slight peaks near 250 (could be because of overexposure/white areas)
-   validates that holdout data has similar lighting + color as training set\
-   important for ensuring model generalizes well, consistent color patterns
-   confirms preprocessing applied correctly --\> RGB is within expected range (0–255)

```{r}
ggplot(all_holdout_long, aes(x = Value, fill = Channel)) +
  geom_density(alpha = 0.5) +
  facet_wrap(~ Channel, scales = "free") +
  labs(title = "Holdout RGB Density", x = "Pixel Value", y = "Density") +
  theme_minimal()
```

-   shows smoothed density distribution of red, green, and blue pixel values in holdout set\
-   similar shape to histograms, but easier to see underlying trends and smooth peaks\
-   blue values concentrated in lower range — many dark blue/gray pixels\
-   green peaks around 120–150, likely from vegetation or green surfaces\
-   red is more evenly distributed, with small peaks across the full range\
-   confirms holdout set has similar structure to training set RGB-wise\
-   smooth curves make it easier to spot mode shifts or color balance issues\
-   no extreme color outliers — suggests preprocessing is consistent\
-   useful for sanity-checking color distributions before applying model\
-   helps visually compare holdout vs training distributions for RGB inputs

```{r}
all_holdout_1 <- all_holdout_1 %>%
  mutate(
    BlueRatio = Blue / (Red + Green + Blue + 1),
    GreenMinusRed = Green - Red,
    BlueMinusRed = Blue - Red
  )

ggplot(all_holdout_1, aes(x = BlueRatio)) +
  geom_density(fill = "blue", alpha = 0.6) +
  labs(title = "Blue Ratio Distribution in Holdout Set", x = "Blue Ratio", y = "Density") +
  theme_minimal()
```

-   density plot showing distribution of BlueRatio in holdout (test) data\

-   BlueRatio = Blue / (Red + Green + Blue + 1) — emphasizes "blueness" in a pixel\

-   single, prominent peak around \~0.25 — most pixels not strongly blue-dominant\

-   smaller secondary bumps between 0.3–0.35 — may indicate some tarp or roof areas\

-   long right tail shows a few pixels with high blue dominance\

-   helps check if BlueRatio patterns in test set match training set\

-   no extreme outliers — BlueRatio remains bounded and stable\

-   important for generalizability — confirms that model inputs (like BlueRatio) behave similarly across both sets\

-   supports BlueRatio as a meaningful engineered feature for distinguishing classes (especially Blue Tarp)

    #### Exploratory Data Analysis on a Both Data Sets

```{r}
train_rgb <- training %>%
  select(Red, Green, Blue) %>%
  mutate(Source = "Training")

holdout_rgb <- all_holdout_1 %>%
  select(Red, Green, Blue) %>%
  mutate(Source = "Holdout")

combined_rgb <- bind_rows(train_rgb, holdout_rgb) %>%
  pivot_longer(cols = c(Red, Green, Blue), names_to = "Channel", values_to = "Value")

ggplot(combined_rgb, aes(x = Value, fill = Source)) +
  geom_density(alpha = 0.5) +
  facet_wrap(~ Channel, scales = "free") +
  labs(title = "RGB Distribution Comparison: Training vs. Holdout", x = "Pixel Value", y = "Density") +
  theme_minimal()
```

-   compares Red, Green, and Blue pixel value distributions across training and holdout sets\
-   helps check for dataset drift or mismatch in RGB characteristics\
-   Blue and Green channels show high overlap — distributions are similar\
-   Red channel shows some divergence — especially at extreme values near 255\
-   holdout has fewer high-red pixels compared to training set\
-   overall, good alignment across sets — suggests preprocessing preserved RGB balance\
-   confirms model trained on this training data is likely to generalize well on holdout data\
-   visual inspection step that validates ML assumption: training and test data come from same distribution\
-   worth noting slight differences for channels — might affect class balance (e.g., rooftop/soil detection)

```{r}
train_features <- training %>%
  select(BlueRatio, GreenMinusRed, BlueMinusRed) %>%
  mutate(Source = "Training")

holdout_features <- all_holdout_1 %>%
  select(BlueRatio, GreenMinusRed, BlueMinusRed) %>%
  mutate(Source = "Holdout")

combined_features <- bind_rows(train_features, holdout_features) %>%
  pivot_longer(cols = c(BlueRatio, GreenMinusRed, BlueMinusRed), 
               names_to = "Feature", values_to = "Value")

ggplot(combined_features, aes(x = Value, fill = Source)) +
  geom_density(alpha = 0.5) +
  facet_wrap(~ Feature, scales = "free") +
  labs(title = "Derived Feature Distribution Comparison", 
       x = "Value", y = "Density") +
  theme_minimal()
```

-   compares training vs. holdout distributions for engineered features: BlueMinusRed, BlueRatio, and GreenMinusRed\
-   verifies whether derived features behave similarly in both datasets — critical for model generalization\
-   BlueRatio shows strong alignment — reassuring since it's a key color-based signal for blue tarp detection\
-   BlueMinusRed and GreenMinusRed also largely match — slight spread differences but no major shift\
-   confirms derived features are stable across datasets\
-   supports our ML assumption that learned color ratios in training should work similarly on unseen data\
-   overall: good indication that these engineered features are reliable inputs for modeling

## Additional Visuals

```{r}
ggplot(training, aes(x = Red, y = Blue, color = Class)) +
  geom_point(alpha = 0.4) +
  labs(title = "Red vs Blue Color Space by Class") +
  theme_minimal()

ggplot(training, aes(x = Green, y = Blue, color = Class)) +
  geom_point(alpha = 0.4) +
  labs(title = "Green vs Blue Color Space by Class") +
  theme_minimal()

ggplot(training, aes(x = Red, y = Green, color = Class)) +
  geom_point(alpha = 0.4) +
  labs(title = "Red vs Green Color Space by Class") +
  theme_minimal()

```

-   Red vs Blue: clearly shows Blue Tarp pixels skew toward higher Blue values
-   Green vs Blue: Vegetation clusters lower in Blue
-   Red vs Green: Rooftop and Soil pixels mostly overlap, but Vegetation again shifts lower in Red
-   confirms multicollinearity exists (high RGB correlation), so derived features or PCA is probably needed

```{r}
library(corrplot)

cor_matrix <- cor(training %>% 
                    select(Red, Green, Blue, BlueRatio, GreenMinusRed, BlueMinusRed) %>% 
                    na.omit())

corrplot(cor_matrix, method = "color", addCoef.col = "black", tl.cex = 0.8)

```

-   visualizes how strongly each feature is correlated with others using Pearson correlation
-   RGB are very highly correlated (\>0.94) which is expected since they come from the same pixels
-   BlueRatio has low correlation with raw channels
-   GreenMinusRed + BlueMinusRed are negatively correlated with Red & Green, positively with Blue
-   BlueMinusRed + GreenMinusRed are strongly correlated with each other (0.77)
-   confirms BlueRatio + one color difference may be useful --\> less redundant for modeling

```{r}
training %>%
  pivot_longer(cols = c(Red, Green, Blue, BlueRatio, BlueMinusRed), names_to = "Feature", values_to = "Value") %>%
  ggplot(aes(x = Class, y = Value, fill = Class)) +
  geom_boxplot() +
  facet_wrap(~ Feature, scales = "free", ncol = 2) +
  labs(title = "Feature Distributions by Class") +
  theme_minimal()
```

-   shows distribution of each feature (RGB + derived) for every class
-   BlueRatio + BlueMinusRed separates Blue Tarp from all other classes well
-   Red and Green show broader spread and some class overlap
-   Vegetation stands out in Red + Blue channels with low values which is expected
-   also useful to spot outliers and skewed distributions within each class

```{r}
train_pca <- prcomp(training[, c("Red", "Green", "Blue")], center = TRUE, scale. = TRUE)
pca_df <- as_tibble(train_pca$x[, 1:2]) %>%
  bind_cols(Class = training$Class)

ggplot(pca_df, aes(x = PC1, y = PC2, color = Class)) +
  geom_point(alpha = 0.4) +
  labs(title = "PCA of RGB Features") +
  theme_minimal()

```

-   PCA reduces RGB data to 2D while preserving most variance
-   Blue Tarp + Vegetation form more distinct clusters
-   Soil, Rooftop, + Various Non-Tarp overlap more

## Data Preparation

-   B1 = Red (R)
-   B2 = Green (G)
-   B3 = Blue (B)

The training dataset was partitioned using an 80/20 split, allocating 80% of the data to the training set and the remaining 20% to the testing set. This proportion was selected due to the substantial size of the original dataset, which contains over sixty thousand observations. As a result, the testing set comprises more than ten thousand observations, providing a robust sample for model validation. Prioritizing a larger training set is expected to enhance model performance by allowing the algorithms to learn from a greater volume of data. Furthermore, since ten-fold cross-validation will be employed during model training, a larger training set ensures that each fold contains a sufficient number of observations. The data split was stratified based on the Tarp variable to maintain consistent proportions of ‘Blue Tarp’ and ‘Not Blue Tarp’ observations across both the training and testing sets

## Model Fitting, Tuning Parameter Selection, Evaluation

As outlined above, the full training set was split into training and testing sets by an 80/20 proportion. A recipe with a pre-processing step for synthetic minority oversampling then each model was created using the same recipe to ensure the models could be compared fairly. The logistic regression model was created using the `glm` engine and the LDA and QDA models were created using the `MASS` engine. The model workflows were then created.

```{r}
## Skip this part as per professor feedback
#haiti_split = initial_split(training,prop = 0.8,strata = Tarp)
#haiti_train = training(haiti_split)
#haiti_test = testing(haiti_split)

```

```{r}
formula <- Tarp ~ Red + Green + Blue
recipe <- recipe(formula,data=training) %>% 
  step_smote(Tarp,seed = 9,over_ratio = tune())
  #step_normalize(all_numeric_predictors()) # Probably can be replaced with SMOTE
```

#### Model Creation

```{r}
resamples <- vfold_cv(training, v=10, strata=Tarp)
custom_metrics <- metric_set(roc_auc, accuracy,f_meas)
cv_control <- control_resamples(save_pred=TRUE,save_workflow = T,event_level = 'first')

## Phase 1
logreg_spec <- logistic_reg(mode='classification', engine='glm')
lda_spec <- discrim_linear(mode = 'classification',engine = 'MASS')
qda_spec <- discrim_quad(mode = 'classification',engine = 'MASS')

# Phase 2 
# K-NEAREST NEIGHBORS (KNN)
knn_spec <- nearest_neighbor(
  neighbors = 4,           
  weight_func = "rectangular",  
  dist_power = 2              
) %>%
  set_mode("classification") %>%
  set_engine("kknn")

# PENALIZED LOGISTIC REGRESSION (ELASTIC NET)
elastic_net_spec <- logistic_reg(
  penalty = tune(),           
  mixture = 0.5            
) %>%
  set_mode("classification") %>%
  set_engine("glmnet")


logreg_wf <- workflow() %>%
    add_recipe(recipe) %>%
    add_model(logreg_spec)

lda_wf <- workflow() %>% 
  add_recipe(recipe) %>% 
  add_model(lda_spec)

qda_wf <- workflow() %>% 
  add_recipe(recipe) %>% 
  add_model(qda_spec)

# KNN workflow
knn_wf <- workflow() %>%
  add_recipe(recipe) %>%
  add_model(knn_spec)

# Elastic Net workflow
elastic_net_wf <- workflow() %>%
  add_recipe(recipe) %>%
  add_model(elastic_net_spec)
```

This code creates a **hyperparameter tuning function specifically for SMOTE over-sampling ratios** and applies it to three LR, LDA, QDA classification models

**Problem Being Solved**:

-   Your earlier results showed **class imbalance** (high accuracy but lower F-measures)

-   This function finds the **optimal SMOTE oversampling ratio** for each model

-   Goal: Improve minority class detection (better F-measure) while maintaining ROC-AUC

**Expected Outcome**:

-   Each model will be tuned to find the best balance between over-sampling and model performance

-   The `lr_results$best`, `lda_results$best`, and `qda_results$best` will contain optimal over-sampling ratios

-   Plots will show how performance varies with different oversampling levels

    **Key Insight**: This is a sophisticated approach to handle class imbalance by **automatically finding the optimal amount of synthetic minority samples** to generate for each classification algorithm!

```{r setup-parallel}

tune_ratio <- function (wf, best_by, modelname, size = 20) {
  parameters <- extract_parameter_set_dials(wf) %>%
    update(over_ratio = over_ratio(c(0,1))
           )
  resamples <- vfold_cv(training, v=10, strata=Tarp)
  custom_metrics <- metric_set(roc_auc,f_meas)
  cv_control <- control_resamples(save_pred=TRUE,save_workflow = T,event_level = 'first')

  tune_results <- tune_grid(wf,
      resamples=resamples,
      control=cv_control,
      grid=grid_regular(parameters, levels=size),
      metrics = custom_metrics)

  best <- select_best(tune_results,metric = best_by)
  plot <- autoplot(tune_results) + ggtitle(modelname)
  
  return(list(best=best,plot=plot))
}

lr_results <- tune_ratio(logreg_wf,best_by = 'f_meas', modelname = "Logistic Regression.")
lda_results <- tune_ratio(lda_wf,best_by = 'f_meas', modelname ='LDA')
qda_results <- tune_ratio(qda_wf,best_by = 'f_meas',modelname = 'QDA')

```

```{r setup-parallel}
cat("Fitting KNN...\n")
knn_results <- tune_ratio(knn_wf,best_by = 'f_meas', modelname = "KNN.")
```

```{r setup-parallel}

elastic_net_results <- tune_ratio(elastic_net_wf,best_by = 'f_meas', modelname = "Elastic Net")
cat("Fitting Elastic Net...\n")
```

```{r}
lr_results$plot + lda_results$plot + qda_results$plot

```

```{r}
knn_results$plot
```

```{r}
elastic_net_results$plot
```

**Logistic Regression:**

-   **F-Measure**: Peaks at ratio 0.925, then **declines steadily** with more over-sampling

-   **ROC-AUC**: **Improves consistently** from 0.0 to 1.0 (99.849% to 99.865%)

-   Minimal/no over-sampling works best for F-measure, but slight over-sampling helps ROC-AUC

**LDA:**

-   **F-Measure**: **Improves dramatically** from 0.0 to 1.0 (75.9% to 76.75%)

-   **ROC-AUC**: **Strong upward trend** (98.9% to 99.4%)

-   LDA **benefits significantly** from over-sampling - handles class imbalance better with synthetic data

**QDA:**

-   **F-Measure**: **Peaks early** (\~91%) at ratio 0.1-0.2, then declines to \~82%

-   **ROC-AUC**: **Slight improvement** then plateaus at \~99.83%

-   Moderate over-sampling (10-20%) optimal for QDA

**KNN :**

-   **K=1**: Worst performance (ROC-AUC \~97%, F-measure \~93.5%)

-   **K=2-4**: **Best performers** (ROC-AUC \~99.3%, F-measure \~95.5-96%)

-   **K=5+**: Declining performance with higher K values

-   **Over-sampling effect**: Minimal impact - performance stable across ratios

-   **Key Finding**: **K=3 or K=4** appears optimal for your Haiti dataset

**ElasticNet Penalty Analysis :**

-   **Low penalties** (1e-08 to 1e-04): **Excellent performance** (F-measure \~80%, ROC-AUC \~99%)

-   **Medium penalties** (1e-04 to 1e-03): **Gradual decline**

-   **High penalties** (\>1e-02): **Dramatic performance collapse** (F-measure drops to \~15-50%)

**Optimal Range**: **1e-06 to 1e-04** for penalty parameter

**Key Insights:**

**Class Imbalance Handling:**

-   **LDA benefits most** from over-sampling

-   **Logistic Regression prefers** minimal over-sampling

-   **QDA works best** with light over-sampling (10-20%)

-   **KNN is robust** to over-sampling variations

**Hyperparameter Sensitivity:**

-   **KNN**: Moderately sensitive (K=2-4 optimal)

-   **ElasticNet**: **Highly sensitive** to penalty (narrow optimal range)

-   **Over-sampling**: **Model-specific** responses

**Optimal Configuration Recommendations:**

**KNN**:

-   **K = 3 or 4** neighbors

-   **Over-sampling ratio**: 0.0-0.2 (minimal impact)

**ElasticNet**:

-   **Penalty**: 1e-05 to 1e-04

-   **Over-sampling ratio**: 0.0-0.2

**LDA**:

-   **Over-sampling ratio**: 0.8-1.0 (maximum benefit)

**QDA**:

-   **Over-sampling ratio**: 0.1-0.2 (sweet spot)

**Logistic Regression**:

-   **Over-sampling ratio**: 0.0-0.1 (minimal over-sampling)

Finally, the models were finalized using the optimal over-sampling ratio, and then trained using ten-fold cross validation, the results are summarized in the table below.

```{r}
logreg_wf <- logreg_wf %>% 
  finalize_workflow(parameters = lr_results$best)
lda_wf <- lda_wf %>% 
  finalize_workflow(parameters = lda_results$best)
qda_wf <- qda_wf %>% 
  finalize_workflow(parameters = qda_results$best)
knn_wf <- knn_wf %>% 
  finalize_workflow(parameters = knn_results$best)
elastic_net_wf <- elastic_net_wf %>% 
  finalize_workflow(parameters = elastic_net_results$best)
```

```{r setup-parallel}

logreg_cv <- fit_resamples(logreg_wf,resamples,metrics = custom_metrics,control = cv_control)
lda_cv <- fit_resamples(lda_wf,resamples,metrics=custom_metrics,control=cv_control)
qda_cv <- fit_resamples(qda_wf,resamples,metrics=custom_metrics,control=cv_control)
knn_cv <- fit_resamples(knn_wf,resamples,metrics=custom_metrics,control=cv_control)
elasticnet_cv <- fit_resamples(elastic_net_wf,resamples,metrics=custom_metrics,control=cv_control)
```

```{r}
logreg_metrics <- collect_metrics(logreg_cv)
lda_metrics <- collect_metrics(lda_cv)
qda_metrics <- collect_metrics(qda_cv)
knn_metrics <- collect_metrics(knn_cv)
elasticnet_metrics <- collect_metrics(elasticnet_cv)

extract_model_metrics <- function(cv_result, model_name) {
  metrics <- collect_metrics(cv_result)
  
  acc_mean <- metrics$mean[metrics$.metric == "accuracy"]
  acc_se <- metrics$std_err[metrics$.metric == "accuracy"]
  f_mean <- metrics$mean[metrics$.metric == "f_meas"]
  f_se <- metrics$std_err[metrics$.metric == "f_meas"]
  roc_mean <- metrics$mean[metrics$.metric == "roc_auc"]
  roc_se <- metrics$std_err[metrics$.metric == "roc_auc"]
  
  return(data.frame(
    model = model_name,
    mean_accuracy = acc_mean,
    mean_f_meas = f_mean,
    mean_roc_auc = roc_mean,
    std_err_accuracy = acc_se,
    std_err_f_meas = f_se,
    std_err_roc_auc = roc_se
  ))
}
cv_metrics <- rbind(
  extract_model_metrics(logreg_cv, 'Logistic Regression'),
  extract_model_metrics(lda_cv, 'LDA'),
  extract_model_metrics(qda_cv, 'QDA'),
  extract_model_metrics(knn_cv, 'KNN'),
  extract_model_metrics(elasticnet_cv, 'ElasticNet')
)

print(cv_metrics)
```


Top Performers:

Logistic Regression leads with 99.57% accuracy, demonstrating that simpler models can be highly effective for your dataset QDA follows closely at 99.49% accuracy KNN achieves 99.72% accuracy with the most consistent performance (lowest standard errors across all metrics)

Performance Patterns: All models show excellent performance with accuracies above 95%, but there are notable differences in consistency. KNN stands out with remarkably low standard errors (0.0002-0.003), indicating very stable predictions across different data splits or cross-validation folds. F1 Score Insights: The F1 scores range from 76.9% (LDA) to 95.7% (KNN), suggesting your dataset may have class imbalance issues. The gap between accuracy and F1 scores indicates that while models predict the majority class well, they vary in their ability to handle minority classes effectively. ROC-AUC Performance: All models achieve ROC-AUC scores above 99%, with KNN again showing the best combination of high performance (99.13%) and low variability. Recommendations:

KNN appears to be your most reliable model with consistent high performance across all metrics Logistic Regression offers excellent interpretability with minimal performance trade-off

**KNN - BEST OVERALL:**

-   **99.72% Accuracy** (highest!)

-   **95.65% F-Measure** (best precision-recall balance)

-   Only **3 errors per 1000 predictions**

**Logistic Regression - 2nd Best overall**

-   **99.86% ROC-AUC** (tied best discrimination)

-   **99.57% Accuracy** (second highest)

-   **Most stable** performance (lowest std errors)

-   **Highly interpretable**

**ElasticNet - 3rd best:**

-   **99.86% ROC-AUC** (tied best)

-   **99.52% Accuracy**

-   **Built-in regularization** prevents overfitting

Key Obeservations

-   **KNN dominates** accuracy and F-measure

-   **Logistic/ElasticNet** excel at ROC-AUC

-   **LDA** underperforms on F-measure (class imbalance issues)

**PRIMARY CHOICE: KNN**

-   **Best overall performance** for tarp detection

-   **Excellent at catching positive cases** (minimal false negatives)

-   **99.7% accuracy** = only 3 errors per 1000 predictions

**Strong Backup: Logistic Regression**

-   **Near-perfect discrimination** (99.86% ROC-AUC)

-   **Highly interpretable** for business stakeholders

-   **Most stable** and reliable performance

```{r}
cv_metrics_long <- cv_metrics %>%
  pivot_longer(
    cols = -model,
    names_to = c(".value", ".metric"),
    names_pattern = "(.*)_(.*)"
  )

ggplot(cv_metrics_long, aes(x = mean, y = reorder(model, mean), 
                           xmin = mean - std_err, xmax = mean + std_err)) +
  geom_point(size = 3, color = "steelblue") +
  geom_linerange(color = "steelblue", size = 1) +
  facet_wrap(~ .metric, scales = "free_x") +
  labs(
    title = "Haiti Tarp Detection - Model Performance",
    x = "Performance Score",
    y = "Model (ranked by performance)"
  ) +
  theme_minimal()
```

```{r}
roc_cv_plot <- function(model_cv, model_name) {
cv_predictions <- collect_predictions(model_cv)
cv_roc <- cv_predictions %>%
roc_curve(truth=Tarp, .pred_Tarp, event_level="first")
g <- autoplot(cv_roc) +
labs(title=model_name)
return(g)
}

g1 <- roc_cv_plot(logreg_cv, "Logistic regression")
g2 <- roc_cv_plot(lda_cv, "LDA")
g3 <- roc_cv_plot(qda_cv, "QDA")
g4 <- roc_cv_plot(knn_cv, "KNN")
g5 <- roc_cv_plot(elasticnet_cv, "Elastic Net")
g1 + g2 + g3
g4 + g5
```

Overlay Plot

```{r}
bind_rows(
collect_predictions(logreg_cv) %>% mutate(model="Logistic regression"),
collect_predictions(lda_cv) %>% mutate(model="LDA"),
collect_predictions(qda_cv) %>% mutate(model="QDA"),
collect_predictions(knn_cv) %>% mutate(model="KNN"),
collect_predictions(elasticnet_cv) %>% mutate(model="Elastic Net")
) %>%
group_by(model) %>%
roc_curve(truth=Tarp, .pred_Tarp, event_level="first") %>%
autoplot()
```

#### Threshold Selection

```{r fig.width=6, fig.height=6}
threshold_scan_cv <- function(model_cv, data, model_name) {
    threshold_data <- model_cv %>% 
      fit_best() %>% 
      augment(data) %>% 
        probably::threshold_perf(truth = Tarp, estimate = .pred_Tarp,
            thresholds=seq(0.05, 0.95, 0.01), event_level="first",
            metrics=metric_set(f_meas))
    opt_threshold <- threshold_data %>%
        arrange(-.estimate) %>%
        first()
    g <- ggplot(threshold_data, aes(x=.threshold, y=.estimate)) +
        geom_line() +
        geom_point(data=opt_threshold, color="red", size=2) +
        labs(title=model_name, x="Threshold", y="F Measurement") +
        coord_cartesian(ylim=c(0.01, 0.9))
    return(list(
        graph=g,
        threshold=opt_threshold %>%
            pull(.threshold)
    ))
}

g1 <- threshold_scan_cv(logreg_cv, training, "Logistic regression")
g2 <- threshold_scan_cv(lda_cv, training, "LDA")
g3 <- threshold_scan_cv(qda_cv, training, "QDA")
g4 <- threshold_scan_cv(knn_cv, training, "KNN")
g5 <- threshold_scan_cv(elasticnet_cv, training, "Elastic Net")

traingraphs <- (g1$graph + g2$graph + g3$graph + g4$graph+ g5$graph ) + plot_annotation(tag_level = 'new')
traingraphs+ plot_annotation(tag_levels = 'A')
```
```{r fig.width=6, fig.height=12}
print(g1$threshold)
print(g2$threshold)
print(g3$threshold)
print(g4$threshold)
print(g5$threshold)
```

# Create final model (Based on Metrics KNN is the best model, ELastic Net and Logistic Regression as 2nd and 3rd best model)


```{r}

str(all_holdout)
str(training)
```

Let's perform this on Test Data

**Test data confirmsKNN as the best choice - it offers better performance with operational robustness.**

Let's create confusion matrix on the test data set - KNN.

```{r}
all_holdout_fixed <- all_holdout %>%
  mutate(Tarp = factor(Tarp, levels = c("yes", "no"), labels = c("Tarp", "Not Tarp")))
```

```{r}
cat("\n KNN \n")
final_model_knn <- knn_wf %>% fit(data = training)
knn_holdout_pred <- predict(final_model_knn, new_data = all_holdout, , type = "prob")


##
##knn_holdout_pred_fixed <- knn_holdout_pred %>%
##  mutate(.pred_class = factor(.pred_class, levels = c("Not Tarp", "Tarp")))

##knn_holdout_results <- bind_cols(all_holdout_fixed, knn_holdout_pred_fixed)

##knn_holdout_results %>% 
##  conf_mat(truth = Tarp, estimate = .pred_class)

```
```{r}
knn_threshold <- 0.26

knn_holdout_custom <- knn_holdout_pred %>%
  mutate(.pred_class = factor(
    ifelse(.pred_Tarp >= knn_threshold, "Tarp", "Not Tarp"),
    levels = c("Tarp", "Not Tarp")
  ))

# Bind with holdout data and create confusion matrix
knn_holdout_results_custom <- bind_cols(all_holdout_fixed, knn_holdout_custom)

knn_holdout_results <- knn_holdout_results_custom %>% 
  conf_mat(truth = Tarp, estimate = .pred_class)

knn_metrics <- knn_holdout_results_custom %>% 
  metric_set(accuracy, sensitivity, specificity, precision, recall, f_meas)(
    truth = Tarp, estimate = .pred_class
  )%>%
  mutate(Model = "KNN")

```
Let's create confusion matrix on the test data set - Elastic Net

```{r}
cat("\n ELastic Net \n")

final_elasticnet_model <- elastic_net_wf %>% fit(data = training)
elasticnet_holdout_pred <- predict(final_elasticnet_model, new_data = all_holdout, type = "prob")
elasticnet_threshold <- 0.36

elasticnet_holdout_pred_fixed <- elasticnet_holdout_pred %>%
  mutate(.pred_class = factor(
    ifelse(.pred_Tarp >= elasticnet_threshold, "Tarp", "Not Tarp"),
    levels = c("Tarp", "Not Tarp")
  ))

elasticnet_holdout_results <- bind_cols(all_holdout_fixed, elasticnet_holdout_pred_fixed)

elasticnet_holdout_results %>% 
  conf_mat(truth = Tarp, estimate = .pred_class)

elasticnet_metrics <- elasticnet_holdout_results %>% 
  metric_set(accuracy, sensitivity, specificity, precision, recall, f_meas)(
    truth = Tarp, estimate = .pred_class
  )%>%
  mutate(Model = "Elastic Net")

```



Let's create confusion matrix on the test data set - Logistic Regression

```{r}
cat("\n Logisitic Regrssion \n")

final_logreg_model <- logreg_wf %>% fit(data = training)
logreg_holdout_pred <- predict(final_logreg_model, new_data = all_holdout, type = "prob")

logreg_threshold <- 0.47

logreg_holdout_pred_fixed <- logreg_holdout_pred %>%
  mutate(.pred_class = factor(
    ifelse(.pred_Tarp >= logreg_threshold, "Tarp", "Not Tarp"),
    levels = c("Tarp", "Not Tarp")
  ))

logreg_holdout_results <- bind_cols(all_holdout_fixed, logreg_holdout_pred_fixed)

logreg_holdout_results %>% 
  conf_mat(truth = Tarp, estimate = .pred_class)

logreg_metrics <-logreg_holdout_results %>% 
  metric_set(accuracy, sensitivity, specificity, precision, recall, f_meas)(
    truth = Tarp, estimate = .pred_class
  )%>%
  mutate(Model = "Logistic regression")
```


```{r}



combined_metrics <- bind_rows(logreg_metrics, elasticnet_metrics,knn_metrics) %>%
  select(Model, .metric, .estimate) %>%
  pivot_wider(names_from = .metric, values_from = .estimate) %>%
  mutate(across(where(is.numeric), ~round(.x, 3)))

print(combined_metrics)

```

Overall Best Model: Elastic Net

Elastic Net provides the best balanced performance across all metrics:

Highest accuracy (98.7%) and best precision (41.2%)
Excellent sensitivity (99.2%) - only missing 0.8% of tarps
Strong specificity (98.7%) - very few false alarms

Model-by-Model Analysis:
1. Logistic Regression:

Strength: Highest sensitivity (99.4%) - catches almost every tarp
Weakness: Poor precision (20.4%) - 4 false alarms for every true tarp

2. Elastic Net:

Strength: Best overall balance - high accuracy with reasonable precision
Trade-off: Slightly lower sensitivity than logistic regression

3. KNN:

Strength: Highest precision (53.6%) and specificity (99.3%) - fewest false alarms
Critical weakness: Lowest sensitivity (88.0%) - misses 12% of actual tarps

Key Performance Insights:

Sensitivity Trade-offs:

Logistic Regression (99.4%) > Elastic Net (99.2%) > KNN (88.0%)
KNN's 12% miss rate could be problematic for emergency response


Precision Rankings:

KNN (53.6%) > Elastic Net (41.2%) > Logistic Regression (20.4%)
Elastic Net offers much better precision than logistic regression with minimal sensitivity loss


F1-Score (balances precision and recall):

KNN (66.7%) > Elastic Net (58.2%) > Logistic Regression (33.8%)


Outcome : Elastic Net is the optimal choice for most real-world applications, providing 99.2% tarp detection with significantly fewer false alarms than logistic regression.





